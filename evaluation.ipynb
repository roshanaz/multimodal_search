{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Initialize SentenceTransformer for manual setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "clip_model = SentenceTransformer('clip-ViT-B-32', device=device, cache_folder='./model_cache')\n",
    "\n",
    "# Define queries\n",
    "queries = [\"bag\", \"dog\", \"park\", \"red car\", \"sunset\"]\n",
    "\n",
    "def is_relevant(caption: str, query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the query term appears in the caption (case-insensitive).\n",
    "    You can enhance this with synonyms, lemmatization, or visual inspection.\n",
    "    \"\"\"\n",
    "    query_terms = query.lower().split()  # Split query into terms (e.g., \"red car\" -> [\"red\", \"car\"])\n",
    "    caption_lower = caption.lower()\n",
    "    return any(re.search(r'\\b' + re.escape(term) + r'\\b', caption_lower) for term in query_terms)\n",
    "\n",
    "\n",
    "def generate_ground_truth(client: weaviate.Client, collection_name: str, queries: List[str]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Search all captions in the collection to find relevant image_ids for each query.\n",
    "    Returns a dict mapping query to set of relevant image_ids.\n",
    "    \"\"\"\n",
    "    ground_truth = {query: set() for query in queries}\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    for obj in tqdm(collection.iterator(), desc=f\"Generating ground truth for {collection_name}\"):\n",
    "        image_id = obj.properties[\"image_id\"]\n",
    "        captions = obj.properties[\"captions\"]\n",
    "        for query in queries:\n",
    "            if is_relevant(captions, query):\n",
    "                ground_truth[query].add(image_id)\n",
    "\n",
    "    for query, relevant_ids in ground_truth.items():\n",
    "        print(f\"Query: {query}, Relevant Images: {len(relevant_ids)}\")\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def calculate_metrics(retrieved_ids: List[str], relevant_ids: Set[str], k: int = 5) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate Precision@k, Recall@k, and Average Precision for a single query.\n",
    "    - retrieved_ids: List of retrieved image_ids (top-k results).\n",
    "    - relevant_ids: Set of ground truth relevant image_ids.\n",
    "    - k: Number of results to consider (e.g., 5).\n",
    "    Returns: (precision, recall, average_precision)\n",
    "    \"\"\"\n",
    "    # Truncate to top-k results\n",
    "    # retrieved_ids = retrieved_ids[:k]\n",
    "    \n",
    "    # Count relevant items in retrieved results\n",
    "    # relevant_retrieved = sum(1 for image_id in retrieved_ids if image_id in relevant_ids)\n",
    "    relevant = set(relevant_ids)\n",
    "    retrieved = set(retrieved_ids[:k])\n",
    "    true_positives = len(relevant & retrieved)\n",
    "\n",
    "    \n",
    "    # Precision@k\n",
    "    precision = true_positives / k if k > 0 else 0.0\n",
    "    \n",
    "    # Recall@k\n",
    "    recall = true_positives / len(relevant_ids) if relevant_ids else 0.0\n",
    "    \n",
    "    average_precision = 0.0\n",
    "    relevant_count = 0\n",
    "    for rank, image_id in enumerate(retrieved_ids, 1):\n",
    "        if image_id in relevant_ids:\n",
    "            relevant_count += 1\n",
    "            precision_at_rank = relevant_count / rank\n",
    "            average_precision += precision_at_rank\n",
    "    average_precision = average_precision / len(relevant_ids) if relevant_ids else 0.0\n",
    "    \n",
    "    return precision, recall, average_precision\n",
    "\n",
    "\n",
    "def evaluate_setup(client: weaviate.Client, collection_name: str, queries: List[str], ground_truth: Dict[str, Set[str]], is_manual: bool = False) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Run queries and calculate metrics for a given setup.\n",
    "    - is_manual: True for SentenceTransformer (near_vector), False for multi2vec-clip (near_text).\n",
    "    Returns: Dict mapping query to metrics (precision, recall, ap, latency, results).\n",
    "    \"\"\"\n",
    "    collection = client.collections.get(collection_name)\n",
    "    response = collection.aggregate.over_all(total_count=True)\n",
    "    print(f\"{collection_name} collection size is: {response.total_count}\")\n",
    "    results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if is_manual:\n",
    "                # Manual setup: Compute query vector and use near_vector\n",
    "                query_vector = clip_model.encode(query, convert_to_numpy=True).tolist()\n",
    "                response = collection.query.near_vector(near_vector=query_vector, limit=5)\n",
    "            else:\n",
    "                # Multi2vec-clip: Use near_text\n",
    "                response = collection.query.near_text(query=query, limit=5)\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            retrieved_ids = [obj.properties[\"image_id\"] for obj in response.objects]\n",
    "            retrieved_captions = [obj.properties[\"captions\"] for obj in response.objects]\n",
    "            \n",
    "            precision, recall, ap = calculate_metrics(retrieved_ids, ground_truth[query], k=5)\n",
    "            \n",
    "            results[query] = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"ap\": ap,\n",
    "                \"latency\": latency,\n",
    "                \"image_ids\": retrieved_ids,\n",
    "                \"captions\": retrieved_captions\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query '{query}' in {collection_name}: {str(e)}\")\n",
    "            results[query] = {\n",
    "                \"precision\": 0.0,\n",
    "                \"recall\": 0.0,\n",
    "                \"ap\": 0.0,\n",
    "                \"latency\": 0.0,\n",
    "                \"image_ids\": [],\n",
    "                \"captions\": []\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    client = weaviate.connect_to_local()\n",
    "    \n",
    "    # Generate ground truth for both collections\n",
    "    ground_truth_manual = generate_ground_truth(client, \"Flickr30k_manual\", queries)\n",
    "    ground_truth_multi2vec = generate_ground_truth(client, \"Flickr30k_multi2vec\", queries)\n",
    "    \n",
    "    # Evaluate both setups\n",
    "    results_manual = evaluate_setup(client, \"Flickr30k_manual\", queries, ground_truth_manual, is_manual=True)\n",
    "    results_multi2vec = evaluate_setup(client, \"Flickr30k_multi2vec\", queries, ground_truth_multi2vec, is_manual=False)\n",
    "    \n",
    "    # Summarize results\n",
    "    print(\"\\nMulti2vec Results:\")\n",
    "    avg_precision_multi2vec = avg_recall_multi2vec = avg_ap_multi2vec = avg_latency_multi2vec = 0.0\n",
    "    for query, metrics in results_multi2vec.items():\n",
    "        print(f\"Query: {query}, Precision@5: {metrics['precision']:.3f}, Recall@5: {metrics['recall']:.3f}, AP@5: {metrics['ap']:.3f}\")\n",
    "        avg_precision_multi2vec += metrics['precision']\n",
    "        avg_recall_multi2vec += metrics['recall']\n",
    "        avg_ap_multi2vec += metrics['ap']\n",
    "        avg_latency_multi2vec += metrics['latency']\n",
    "\n",
    "    # avg_precision_multi2vec /= len(queries)\n",
    "    # avg_recall_multi2vec /= len(queries)\n",
    "    # avg_ap_multi2vec /= len(queries)\n",
    "    # avg_latency_multi2vec /= len(queries)\n",
    "    \n",
    "    # print(f\"\\nAverage Metrics (Multi2vec-clip):\")\n",
    "    # print(f\"  Avg Precision@5: {avg_precision_multi2vec:.3f}\")\n",
    "    # print(f\"  Avg Recall@5: {avg_recall_multi2vec:.3f}\")\n",
    "    # print(f\"  mAP: {avg_ap_multi2vec:.3f}\")\n",
    "    # print(f\"  Avg Latency: {avg_latency_multi2vec:.3f}s\")\n",
    "\n",
    "    print(\"\\nManual Results:\")\n",
    "    avg_precision_manual = avg_recall_manual = avg_ap_manual = avg_latency_manual = 0.0\n",
    "    for query, metrics in results_manual.items():\n",
    "        print(f\"Query: {query}, Precision@5: {metrics['precision']:.3f}, Recall@5: {metrics['recall']:.3f}, AP@5: {metrics['ap']:.3f}\")\n",
    "        avg_precision_manual += metrics['precision']\n",
    "        avg_recall_manual += metrics['recall']\n",
    "        avg_ap_manual += metrics['ap']\n",
    "        avg_latency_manual += metrics['latency']\n",
    "\n",
    "    # avg_precision_manual /= len(queries)\n",
    "    # avg_recall_manual /= len(queries)\n",
    "    # avg_ap_manual /= len(queries)\n",
    "    # avg_latency_manual /= len(queries)\n",
    "    \n",
    "    # print(f\"\\nAverage Metrics (Manual):\")\n",
    "    # print(f\"  Avg Precision@5: {avg_precision_manual:.3f}\")\n",
    "    # print(f\"  Avg Recall@5: {avg_recall_manual:.3f}\")\n",
    "    # print(f\"  mAP: {avg_ap_manual:.3f}\")\n",
    "    # print(f\"  Avg Latency: {avg_latency_manual:.3f}s\")\n",
    "\n",
    "    \n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ground truth for Flickr30k_manual: 598it [00:00, 1336.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: bag, Relevant Images: 8\n",
      "Query: dog, Relevant Images: 36\n",
      "Query: park, Relevant Images: 12\n",
      "Query: red car, Relevant Images: 99\n",
      "Query: sunset, Relevant Images: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ground truth for Flickr30k_multi2vec: 598it [00:00, 2466.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: bag, Relevant Images: 8\n",
      "Query: dog, Relevant Images: 36\n",
      "Query: park, Relevant Images: 12\n",
      "Query: red car, Relevant Images: 99\n",
      "Query: sunset, Relevant Images: 2\n",
      "Flickr30k_manual collection size is: 598\n",
      "Flickr30k_multi2vec collection size is: 598\n",
      "\n",
      "Multi2vec Results:\n",
      "Query: bag, Precision@5: 0.200, Recall@5: 0.125, AP@5: 0.031\n",
      "Query: dog, Precision@5: 1.000, Recall@5: 0.139, AP@5: 0.139\n",
      "Query: park, Precision@5: 0.000, Recall@5: 0.000, AP@5: 0.000\n",
      "Query: red car, Precision@5: 0.200, Recall@5: 0.010, AP@5: 0.002\n",
      "Query: sunset, Precision@5: 0.000, Recall@5: 0.000, AP@5: 0.000\n",
      "\n",
      "Manual Results:\n",
      "Query: bag, Precision@5: 0.000, Recall@5: 0.000, AP@5: 0.000\n",
      "Query: dog, Precision@5: 0.800, Recall@5: 0.111, AP@5: 0.106\n",
      "Query: park, Precision@5: 0.000, Recall@5: 0.000, AP@5: 0.000\n",
      "Query: red car, Precision@5: 0.400, Recall@5: 0.020, AP@5: 0.014\n",
      "Query: sunset, Precision@5: 0.000, Recall@5: 0.000, AP@5: 0.000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_comparison",
   "language": "python",
   "name": "multimodal_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
