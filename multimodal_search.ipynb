{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install weaviate-client sentence-transformers pillow numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roshanakzakizadeh/Projects/multimodal_comparison/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Multi2VecField\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Initialize CLIP model\n",
    "clip_model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_data(image_path):\n",
    "    \"\"\"Read image as base64 and PIL object, caching to avoid redundant reads\"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        raw_data = img_file.read()\n",
    "        base64_data = base64.b64encode(raw_data).decode(\"utf-8\")\n",
    "        pil_img = Image.open(io.BytesIO(raw_data))\n",
    "    return base64_data, pil_img\n",
    "\n",
    "def process_metadata(metadata, max_chars=200):\n",
    "    \"\"\"Process captions from metadata\"\"\"\n",
    "    captions = metadata.get(\"captions\", [])\n",
    "    concatenated = \" \".join(captions)[:max_chars] if captions else \"No captions available\"\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flickr_schema_multi2vec(client, collection_name, image_weight=0.5):\n",
    "    \"\"\"Create schema for multi2vec-clip\"\"\"\n",
    "    if client.collections.exists(collection_name):\n",
    "        client.collections.delete(collection_name)\n",
    "        print(f\"existing {collection_name} deleted, creating a new one.\")\n",
    "    \n",
    "    image_weight = round(image_weight, 2)\n",
    "    text_weight = round(1.0 - image_weight, 2)\n",
    "\n",
    "    client.collections.create(\n",
    "        collection_name,\n",
    "        properties=[\n",
    "            Property(name=\"image\", data_type=DataType.BLOB),\n",
    "            Property(name=\"image_id\", data_type=DataType.TEXT),\n",
    "            Property(name=\"captions\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "        vectorizer_config=[\n",
    "            Configure.NamedVectors.multi2vec_clip(\n",
    "                name=\"image_vector\",\n",
    "                image_fields=[Multi2VecField(name=\"image\", weight=image_weight)],\n",
    "                text_fields=[Multi2VecField(name=\"captions\", weight=text_weight)]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Created {collection_name} schema\")\n",
    "\n",
    "def create_flickr_schema_manual(client, collection_name):\n",
    "    \"\"\"Create schema for manual vectorization\"\"\"\n",
    "    if client.collections.exists(collection_name):\n",
    "        client.collections.delete(collection_name)\n",
    "        print(f\"existing {collection_name} deleted, creating a new one.\")\n",
    "        \n",
    "    client.collections.create(\n",
    "        collection_name,\n",
    "        properties=[\n",
    "            Property(name=\"image\", data_type=DataType.BLOB),\n",
    "            Property(name=\"image_id\", data_type=DataType.TEXT),\n",
    "            Property(name=\"captions\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "        vectorizer_config=[Configure.NamedVectors.none(name=\"image_vector\")]\n",
    "    )\n",
    "    print(f\"Created {collection_name} schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_multi2vec(client, data_dir, collection_name, batch_size=100):\n",
    "    \"\"\"Import data with multi2vec-clip\"\"\"\n",
    "    collection = client.collections.get(collection_name)\n",
    "    images_dir = os.path.join(data_dir, \"images\")\n",
    "    metadata_dir = os.path.join(data_dir, \"metadata\")\n",
    "    metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]\n",
    "    print(f\"Importing {len(metadata_files)} images for multi2vec\")\n",
    "\n",
    "    batch_errors = []\n",
    "    with collection.batch.fixed_size(batch_size=batch_size) as batch:\n",
    "        for metadata_file in tqdm(metadata_files):\n",
    "            try:\n",
    "                with open(os.path.join(metadata_dir, metadata_file), 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                image_path = os.path.join(images_dir, f\"image_{metadata['image_id']}.jpg\")\n",
    "                image_data, _ = read_image_data(image_path)\n",
    "                captions = process_metadata(metadata)\n",
    "                \n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"image\": image_data,\n",
    "                        \"image_id\": metadata[\"image_id\"],\n",
    "                        \"captions\": captions\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                batch_errors.append(f\"Error processing {metadata_file}: {str(e)}\")\n",
    "                continue\n",
    "    if batch_errors:\n",
    "        print(f\"Encountered {len(batch_errors)} errors during batch import\")\n",
    "        for err in batch_errors[:5]:  # Print first 5 errors\n",
    "            print(err)\n",
    "    print(\"Finished multi2vec import\")\n",
    "\n",
    "def import_data_manual(client, data_dir, collection_name, image_weight = 0.5, batch_size=100):\n",
    "    \"\"\"Import data with manual CLIP vectorization\"\"\"\n",
    "    collection = client.collections.get(collection_name)\n",
    "    images_dir = os.path.join(data_dir, \"images\")\n",
    "    metadata_dir = os.path.join(data_dir, \"metadata\")\n",
    "    metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]\n",
    "    print(f\"Importing {len(metadata_files)} images for manual\")\n",
    "\n",
    "    image_weight = round(image_weight, 2)\n",
    "    text_weight = round(1.0 - image_weight, 2)\n",
    "\n",
    "    batch_errors = []\n",
    "    with collection.batch.fixed_size(batch_size=batch_size) as batch:\n",
    "        for metadata_file in tqdm(metadata_files):\n",
    "            try:\n",
    "                with open(os.path.join(metadata_dir, metadata_file), 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                image_path = os.path.join(images_dir, f\"image_{metadata['image_id']}.jpg\")\n",
    "                image_data, img = read_image_data(image_path)\n",
    "                captions = process_metadata(metadata)\n",
    "                \n",
    "                image_embedding = clip_model.encode(img, convert_to_numpy=True)\n",
    "                caption_list = metadata.get(\"captions\", [])\n",
    "                text_embedding = clip_model.encode(caption_list, convert_to_numpy=True).mean(axis=0) if caption_list else np.zeros_like(image_embedding)\n",
    "                combined_embedding = image_weight * image_embedding + text_weight * text_embedding\n",
    "                \n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"image\": image_data,\n",
    "                        \"image_id\": metadata[\"image_id\"],\n",
    "                        \"captions\": captions\n",
    "                    },\n",
    "                    vector={\"image_vector\": combined_embedding.tolist()}\n",
    "                )\n",
    "                img.close() \n",
    "            except Exception as e:\n",
    "                batch_errors.append(f\"Error processing {metadata_file}: {str(e)}\")\n",
    "                continue\n",
    "    if batch_errors:\n",
    "        print(f\"Encountered {len(batch_errors)} errors during batch import\")\n",
    "        for err in batch_errors[:5]:\n",
    "            print(err)\n",
    "    print(\"Finished manual import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing Flickr30k_manual_50_50 deleted, creating a new one.\n",
      "Created Flickr30k_manual_50_50 schema\n",
      "existing Flickr30k_multi2vec_50_50 deleted, creating a new one.\n",
      "Created Flickr30k_multi2vec_50_50 schema\n",
      "Importing 598 images for manual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [02:00<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished manual import\n",
      "Importing 598 images for multi2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:35<00:00, 16.80it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished multi2vec import\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Connect to Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "\n",
    "    data_dir = \"flickr30k_sample\"\n",
    "    \n",
    "    create_flickr_schema_manual(client, collection_name=\"Flickr30k_manual_50_50\")\n",
    "    create_flickr_schema_multi2vec(client, collection_name=\"Flickr30k_multi2vec_50_50\", image_weight=0.5)\n",
    "\n",
    "    import_data_manual(client, data_dir, collection_name=\"Flickr30k_manual_50_50\", image_weight=0.5)\n",
    "    import_data_multi2vec(client, data_dir, collection_name=\"Flickr30k_multi2vec_50_50\")\n",
    "\n",
    "    client.close ()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_comparison",
   "language": "python",
   "name": "multimodal_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
